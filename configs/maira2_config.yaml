# VERDICT - MAIRA-2 Configuration
# Specific configuration for Microsoft MAIRA-2 model experiments

# Inherit from base configuration
_base_: "base_config.yaml"

# ============================================
# MAIRA-2 Model Settings
# ============================================
model:
  name: "maira-2"
  checkpoint: "microsoft/maira-2"
  
  # MAIRA-2 specific settings
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: true
  trust_remote_code: true
  
  # MAIRA-2 uses RAD-DINO for vision
  vision_encoder:
    name: "rad-dino"
    image_size: 518
    patch_size: 14
    hidden_size: 1024
    num_layers: 24
    num_attention_heads: 16
    freeze: true
    # Layers for multi-scale feature extraction
    layer_extraction: [6, 12, 18, 24]
  
  # MAIRA-2 uses Vicuna-7B as language backbone
  language_model:
    name: "vicuna-7b"
    hidden_size: 4096
    num_layers: 32
    num_attention_heads: 32
    freeze: false
  
  # Generation settings for radiology reports
  generation:
    max_new_tokens: 512
    min_new_tokens: 50
    num_beams: 4
    temperature: 1.0
    top_p: 1.0
    do_sample: false
    repetition_penalty: 1.0
    length_penalty: 1.0
    early_stopping: true
    # Radiology-specific tokens
    bad_words_ids: null
    force_words_ids: null
  
  # Prompt templates for MAIRA-2
  prompts:
    findings_generation: |
      <image>
      Describe the findings in this chest X-ray.
    
    impression_generation: |
      <image>
      Provide a clinical impression for this chest X-ray.
    
    vqa_template: |
      <image>
      Question: {question}
      Answer:
    
    grounded_reporting: |
      <image>
      Generate a findings section for this chest X-ray with anatomical grounding.

# ============================================
# Data Configuration for MAIRA-2
# ============================================
data:
  # MAIRA-2 image preprocessing
  image_size: [518, 518]
  
  # MAIRA-2 specific normalisation
  normalise: true
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  
  # Preprocessing pipeline
  preprocessing:
    resize_mode: "shortest_edge"
    center_crop: false
    pad_to_square: true
    pad_value: 0

# ============================================
# Training Configuration
# ============================================
training:
  # Lower learning rate for fine-tuning
  learning_rate: 1.0e-6
  
  # LoRA configuration (if using parameter-efficient fine-tuning)
  use_lora: false
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules:
      - "q_proj"
      - "v_proj"
      - "k_proj"
      - "o_proj"

# ============================================
# Attribution Configuration for MAIRA-2
# ============================================
attribution:
  # Vision encoder attribution
  vision_attribution:
    method: "integrated_gradients"
    target_layers:
      - "vision_encoder.blocks.6"
      - "vision_encoder.blocks.12"
      - "vision_encoder.blocks.18"
      - "vision_encoder.blocks.23"
    aggregate_method: "mean"
  
  # Cross-modal attribution
  cross_modal:
    compute: true
    attention_heads: "all"
    layer_range: [0, 32]
  
  # Token-level attribution
  token_attribution:
    method: "attention_rollout"
    include_cls_token: true
    normalise: true
